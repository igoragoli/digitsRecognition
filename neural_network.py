"""
neural_network.py

Implements a simple L-layer neural network.
"""

import numpy as np 

class NeuralNetwork():
    """
    Represents a simple L-layer neural network.

    The network is composed of L-1 hidden layers with the ReLU function
    as the activation function, and the output layer has the sigmoid
    function as the activation function.

    Attributes
    ----------
    layers_dims : list
        Dimensions of each layer in the network.
    parameters : dict
        Weights and biases "Wl" and "bl".
    L : int
        # of layers of the network.
    """   

    def __init__(self, layers_dims):
        """
        Constructor method.

        Initializes all attributes. The weights are initialized with small random 
        values, and the biases are initialized with zeros.
        """
        
        self.layers_dims = layers_dims
        self.parameters = {}
        self.L = len(self.layers_dims)

        for l in range(1, self.L):
            self.parameters['W' + str(l)] = np.random.randn(self.layers_dims[l], self.layers_dims[l-1]) * np.sqrt(2 / self.layers_dims[l])
            self.parameters['b' + str(l)] = np.zeros((self.layers_dims[l], 1))

    def forwardprop(self, X):
        """
        Implements forward propagation.
        
        Parameters
        ----------
        X: numpy.array
            Matrix of shape (n_x, m) containing n_x input features for 
            m training examples.

        Returns
        -------
        AL: numpy.array
            Output of the neural network.
        caches: list
            Caches generated by every forward propagation module l, containing
            the tuple (A^{[l-1]}, W^{[l]}, b^{[l]}, Z^{[l]}).
        """
        caches = []
        A = X

        # Hidden layers (ReLU)
        for l in range(1, self.L):
            A_prev = A
            W = self.parameters['W' + str(l)]
            b = self.parameters['W' + str(l)]
            Z = np.dot(W, A_prev) + b
            A = self.g(Z, "relu")
            caches = caches.append((A_prev, W, b, Z))
        
        # Output layer (Sigmoid)
        A_prev = A
        WL = self.parameters['W' + str(self.L)]
        bL = self.parameters['b' + str(self.L)]
        ZL = np.dot(W, A_prev) + bL
        AL = self.g(ZL, "sigmoid")
        caches = caches.append((A_prev, WL, bL, ZL))

        return AL, caches

    def backprop(self, AL, Y, caches):
        """
        Implements backpropagation.

        Parameters
        ----------
        AL : numpy.array
            Output of the neural network.
        Y : numpy.array
            Expected output (labels) of the neural network.
        caches : list
            Caches returned by forwardprop(.).
        
        Returns
        -------
        grads : dict
            Dictionary containing the "dWl" and "dbl' partial derivatives 
            of the cost function with respect to Wl and bl.
        """
        grads = {}
        m = AL.shape[1]
        dAL = AL - Y

        # Output Layer
        A_prev, WL, _, ZL = caches[self.L - 1] 
        dZL = dAL * self.dg(ZL, "sigmoid")
        grads["dW" + str(self.L)]  = (1 / m) * np.dot(dZL, A_prev.T)
        grads["db" + str(self.L)] = (1 / m) * np.sum(dZL, axis=1, keepdims=True)
        grads["dA" + str(self.L - 1)] = np.dot(WL.T, dZL)
    
        # Hidden Layers
        for l in reversed(range(self.L - 1)):
            A_prev, W, _, Z = caches[l]
            dZ = grads["dA" + str(l)] * self.dg(Z, "relu")
            grads["dW" + str(l)]  = (1 / m) * np.dot(dZ, A_prev.T)
            grads["db" + str(l)]  = (1 / m) * np.sum(dZ, axis=1, keepdims=True)
            grads["dA" + str(l - 1)] = np.dot(W.T, dZ)

        return grads

    # TODO: implement cost(.) function

    def g(self, z, activation):
        # TODO: document function.
        if activation == "sigmoid":
            r = 1 / (1 + np.exp(-z))
        elif activation == "relu":
            if z >= 0:
                r = z
            else:
                r = 0
        return r

    def dg(self, z, activation):
        # TODO: document function.
        if activation == "sigmoid":
            sig = self.g(z, "sigmoid")
            r = sig * (1 - sig)
        elif activation == "relu":
            if z >= 0:
                r = 1
            else:
                r = 0
        return r
"""
neural_network.py

Implements a simple L-layer neural network.
"""

import numpy as np 
import matplotlib.pyplot as plt

class NeuralNetwork():
    """
    Represents a simple L-layer neural network.

    The network is composed of L-1 hidden layers with the ReLU function
    as the activation function, and the output layer has the sigmoid
    function as the activation function.

    Attributes
    ----------
    layers_dims : list
        Dimensions of each layer in the network.
    parameters : dict
        Weights and biases "Wl" and "bl".
    L : int
        # of layers of the network.
    """   

    def __init__(self, X, Y, layers_dims, learning_rate = 0.005, iterations = 500):
        """
        Constructor method.
        Initializes all attributes, sets up a L-layer neural network model, 
        and trains the model. The weights are initialized with small random 
        values, and the biases are initialized with zeros.

        Parameters
        ----------
        X : np.array of shape (n^{[0]}, m)
            Input matrix.
        Y : np.array of shape (n^{[L]}, m)
            Output matrix.
        layers_dims : list
            Dimensions of each layer in the network.
        learning_rate : float
            Gradient descent learning rate.
        iterations : int
            Number of iterations for the learning process.
        """
        
        self.layers_dims = layers_dims
        self.parameters = {}
        self.L = len(self.layers_dims)
        costs = []

        for l in range(1, self.L):
            self.parameters['W' + str(l)] = np.random.randn(self.layers_dims[l], self.layers_dims[l-1]) * np.sqrt(2 / self.layers_dims[l])
            self.parameters['b' + str(l)] = np.zeros((self.layers_dims[l], 1))
        
        # Gradient descent.
        for _ in range(iterations):
            # Perform forward propagation.
            AL, caches = self.forwardprop(X)

            # Compute cost.
            J = self.compute_cost(AL, Y)

            # Perform backward propagation.
            grads = self.backprop(AL, Y, caches)

            # Update parameters.
            self.update_parameters(grads, learning_rate)

            costs = costs.append(J)
        
        plt.plot(np.squeeze(costs))
        plt.show()
    
    def forwardprop(self, X):
        """
        Implements forward propagation.
        
        Parameters
        ----------
        X: numpy.array
            Matrix of shape (n_x, m) containing n_x input features for 
            m training examples.

        Returns
        -------
        AL: numpy.array
            Output of the neural network.
        caches: list
            Caches generated by every forward propagation module l, containing
            the tuple (A^{[l-1]}, W^{[l]}, b^{[l]}, Z^{[l]}).
        """
        caches = []
        A = X

        # Hidden layers (ReLU)
        for l in range(1, self.L):
            A_prev = A
            W = self.parameters['W' + str(l)]
            b = self.parameters['b' + str(l)]
            Z = np.dot(W, A_prev) + b
            A = self.g(Z, "relu")
            # TODO: Fix NoneType append problem
            caches = caches.append((A_prev, W, b, Z))
        
        # Output layer (Sigmoid)
        A_prev = A
        WL = self.parameters['W' + str(self.L)]
        bL = self.parameters['b' + str(self.L)]
        ZL = np.dot(W, A_prev) + bL
        AL = self.g(ZL, "sigmoid")
        caches = caches.append((A_prev, WL, bL, ZL))

        return AL, caches

    def backprop(self, AL, Y, caches):
        """
        Implements backpropagation.

        Parameters
        ----------
        AL : numpy.array
            Output of the neural network.
        Y : numpy.array
            Expected output (labels) of the neural network.
        caches : list
            Caches returned by forwardprop().
        
        Returns
        -------
        grads : dict
            Dictionary containing the "dWl" and "dbl' partial derivatives 
            of the cost function with respect to Wl and bl.
        """
        grads = {}
        m = AL.shape[1]
        dAL = AL - Y

        # Output Layer
        A_prev, WL, _, ZL = caches[self.L - 1] 
        dZL = dAL * self.dg(ZL, "sigmoid")
        grads["dW" + str(self.L)]  = (1 / m) * np.dot(dZL, A_prev.T)
        grads["db" + str(self.L)] = (1 / m) * np.sum(dZL, axis=1, keepdims=True)
        grads["dA" + str(self.L - 1)] = np.dot(WL.T, dZL)
    
        # Hidden Layers
        for l in reversed(range(self.L - 1)):
            A_prev, W, _, Z = caches[l]
            dZ = grads["dA" + str(l)] * self.dg(Z, "relu")
            grads["dW" + str(l)]  = (1 / m) * np.dot(dZ, A_prev.T)
            grads["db" + str(l)]  = (1 / m) * np.sum(dZ, axis=1, keepdims=True)
            grads["dA" + str(l - 1)] = np.dot(W.T, dZ)

        return grads

    def compute_cost(self, A, Y):
        """
        Computes the cost function.

        The cost function used in this neural network model is the
        mean squared error (MSE):
        J(A, Y) = \frac{1}{2m} \sum_{i = 1}^{i = m} ||A^{(i)} - Y^{(i)}||^2
        The MSE is divided by 2 so that its derivative with respect to A is easier to compute.

        Parameters
        ----------
        A : numpy.array
            Output of the neural network.
        Y : numpy.array
            Expected output (labels) of the neural network.

        Returns
        -------
        J : float
            Value of the cost function.
        """
        m = A.shape[1]
        J = 1 / (2 * m) * np.sum(np.linalg.norm(A - Y, axis = 0))
        J = np.squeeze(J)
        return J

    def update_parameters(self, grads, learning_rate):
        """
        Updates parameters with gradient descent.

        Parameters
        ----------
        grads : dict
            Dictionary containing the "dWl" and "dbl' partial derivatives 
            of the cost function with respect to Wl and bl.

        learning_rate : float
            Gradient descent learning rate.
        """
        for l in range(1, self.L):
            self.parameters["W" + str(l)] = self.parameters["W" + str(l)] - learning_rate * grads["dW" + str(l)]
            self.parameters["b" + str(l)] = self.parameters["b" + str(l)] - learning_rate * grads["db" + str(l)] 

    def g(self, z, activation):
        # TODO: document function.
        if activation == "sigmoid":
            r = 1 / (1 + np.exp(-z))
        elif activation == "relu":
            r = np.maximum(z, 0)
        return r

    def dg(self, z, activation):
        # TODO: document function.
        if activation == "sigmoid":
            sig = self.g(z, "sigmoid")
            r = sig * (1 - sig)
        elif activation == "relu":
            r = (z >= 0).astype(int)
        return r
    




